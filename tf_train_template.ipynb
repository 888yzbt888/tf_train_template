{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "Simple tf training template.\n",
    "\n",
    "Data generated from pyspark\n",
    "```python\n",
    "df = spark.createDataFrame([('user_a','18','male','tommy','film,music',4,180.3,'shenzhen',1,3.2,),\n",
    "                           ('user_b','16','female','sammy','animation,music',0,166.8,'beijing',0,0.0,),\n",
    "                           ('user_c','22','','raymond','',1,0.0,'',1,5.0,)],\n",
    "                           ['user_id','age','gender','nickname','cates','active_days','height','city','click','duration'])\n",
    "df.write.mode('overwrite').format(\"tfrecords\") \\\n",
    "    .option(\"recordType\", \"Example\") \\\n",
    "    .option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\") \\\n",
    "    .save(\"hdfs://cluster/path\")\n",
    "```\n",
    "| user_id | age | gender | nickname | cate_list        | active_days | height | city   | click | duration |\n",
    "| ----  | ----  | ----   | ----     | ----             | ----        | ----   | ----   | ----  | ----     |\n",
    "|user_a |18     |male    |tommy     |[film, music]     |4            |180.3   |shenzhen|1      |3.2       |\n",
    "|user_b |16     |female  |sammy     |[animation, music]|0            |166.8   |beijing |0      |0.0       |\n",
    "|user_c |22     | \\N     |raymond   |[]                |1            |0.0     | \\N     |1      |5.0       |\n",
    "\n",
    "Under Python3.9, Tensorflow 2.8, Anaconda, Mac"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. config setting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  5 2022, 01:53:17) \n",
      "[Clang 12.0.0 ]\n",
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "print(sys.version)\n",
    "print(tf.__version__)\n",
    "\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "print(cpus)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 train configs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# feature_config = {\n",
    "#     'hash_sample': {\n",
    "#         'preprocessor': 'hash',  # probabilistic collision\n",
    "#         'num_bins': 10_000_000,\n",
    "#         'embedding_size': 8,\n",
    "#         'embedding_type': 'tfra', # tensorflow recommenders addons, dynamic embedding contributed by weixin, ref: https://github.com/tensorflow/recommenders-addons/blob/master/rfcs/20200424-sparse-domain-isolation.md\n",
    "#     },\n",
    "#     'int_sample': {\n",
    "#         'preprocessor': 'int',\n",
    "#         'vocabulary': 'vocab/int_sample.vocab',\n",
    "#         'max_tokens': None,\n",
    "#         'num_oov_indices': 1,\n",
    "#         'embedding_size': 2,\n",
    "#     },\n",
    "#     'norm_sample': {\n",
    "#         'preprocessor': 'norm',\n",
    "#         'mean': 36742.3,\n",
    "#         'variance': 1.3353073e+09,\n",
    "#     },\n",
    "#     'discrete_sample': {\n",
    "#         'preprocessor': 'dis',\n",
    "#         'num_bins': 6, # eq to len(bin_boundaries)+1\n",
    "#         'embedding_size': 8,\n",
    "#         'bin_boundaries': [6246, 7616, 8824, 9915, 10031],\n",
    "#     },\n",
    "#     'discrete_sample_from_existed_float': {\n",
    "#         'feature_name': 'norm_sample', # relate to raw input name, which can convert to different logic features\n",
    "#         'preprocessor': 'dis',\n",
    "#         'num_bins': 6, # eq to len(bin_boundaries)+1\n",
    "#         'embedding_size': 8,\n",
    "#         'bin_boundaries': [6246, 7616, 8824, 9915, 10031],\n",
    "#     },\n",
    "#     'float_sample': {\n",
    "#         'preprocessor': 'float',\n",
    "#     },\n",
    "#     'string_sample': {\n",
    "#         'preprocessor': 'string', # no collision in vocab, but vocab size limit\n",
    "#         'vocabulary': 'vocab/string_sample.vocab',\n",
    "#         'max_tokens': None,\n",
    "#         'num_oov_indices': 1000,\n",
    "#         'embedding_size': 8,\n",
    "#     },\n",
    "#     'shared_embedding_sample': {\n",
    "#         'preprocessor' : 'shared_embedding',\n",
    "#         'shared_embedding': 'hash_sample',\n",
    "#         'pooling': True,\n",
    "#     },\n",
    "# }\n",
    "feature_config = {\n",
    "    'user_id': {\n",
    "        'preprocessor': 'hash',  # probabilistic collision\n",
    "        'num_bins': 100,\n",
    "        'mask_value': None,\n",
    "        'embedding_size': 8,\n",
    "        # 'embedding_type': 'tfra', # tensorflow recommenders addons, dynamic embedding contributed by weixin, ref: https://github.com/tensorflow/recommenders-addons/blob/master/rfcs/20200424-sparse-domain-isolation.md\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"user_id\"),\n",
    "    },\n",
    "    'age': {\n",
    "        'preprocessor': 'string',\n",
    "        'vocabulary': '../vocab/test/age.vocab',\n",
    "        'max_tokens': None,\n",
    "        'num_oov_indices': 1,\n",
    "        'embedding_size': 2,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"age\"),\n",
    "    },\n",
    "    'height_norm': {\n",
    "        'feature_name': 'height',\n",
    "        'preprocessor': 'norm',\n",
    "        'mean': 170.0,\n",
    "        'variance': 1.0,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.float32, name=\"height_norm\"),\n",
    "    },\n",
    "    'height_dis': {\n",
    "        'feature_name': 'height',\n",
    "        'preprocessor': 'dis',\n",
    "        'num_bins': 5, # eq to len(bin_boundaries)+1\n",
    "        'embedding_size': 8,\n",
    "        'bin_boundaries': [150, 160, 170, 180],\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"height_dis\"),\n",
    "    },\n",
    "    'active_days_dis': {\n",
    "        'feature_name': 'active_days',\n",
    "        'preprocessor': 'dis',\n",
    "        'num_bins': 5, # eq to len(bin_boundaries)+1\n",
    "        'embedding_size': 8,\n",
    "        'bin_boundaries': [0, 2, 4, 6],\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"active_days_dis\"),\n",
    "    },\n",
    "    # 'city': {\n",
    "    #     'preprocessor': 'string', # no collision in vocab, but vocab size limit\n",
    "    #     'vocabulary': '../vocab/test/city.vocab',\n",
    "    #     'max_tokens': None,\n",
    "    #     'num_oov_indices': 1000,\n",
    "    #     'embedding_size': 8,\n",
    "    # },\n",
    "    # 'height': {\n",
    "    #     'preprocessor': 'float',\n",
    "    # },\n",
    "    'city': {\n",
    "        'preprocessor': 'hash',  # probabilistic collision\n",
    "        'num_bins': 100,\n",
    "        'mask_value': None,\n",
    "        'embedding_size': 8,\n",
    "        # 'embedding_type': 'tfra', # tensorflow recommenders addons, dynamic embedding contributed by weixin, ref: https://github.com/tensorflow/recommenders-addons/blob/master/rfcs/20200424-sparse-domain-isolation.md\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"city\"),\n",
    "    },\n",
    "    'cate_list': {\n",
    "        # 'feature_name': 'cate_list',\n",
    "        'preprocessor': 'string', # no collision in vocab, but vocab size limit\n",
    "        'vocabulary': '../vocab/test/cates.vocab',\n",
    "        'max_tokens': None,\n",
    "        'num_oov_indices': 1000,\n",
    "        'embedding_size': 8,\n",
    "        'pooling': True,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,None], dtype=tf.int64, name=\"cate_list\"),\n",
    "    },\n",
    "    'cate_list_hash': {\n",
    "        'feature_name': 'cate_list',\n",
    "        'preprocessor': 'hash',  # probabilistic collision\n",
    "        'num_bins': 100,\n",
    "        'mask_value': None,\n",
    "        'embedding_size': 8,\n",
    "        'pooling': True,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,None], dtype=tf.int64, name=\"cate_list_hash\"),\n",
    "    },\n",
    "    'gender': {\n",
    "        'preprocessor': 'string', # no collision in vocab, but vocab size limit\n",
    "        'vocabulary': '../vocab/test/gender.vocab',\n",
    "        'max_tokens': None,\n",
    "        'num_oov_indices': 1000,\n",
    "        'embedding_size': 8,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"gender\"),\n",
    "    },\n",
    "    # 'shared_embedding_sample': {\n",
    "    #     'preprocessor' : 'shared_embedding',\n",
    "    #     'shared_embedding': 'hash_sample',\n",
    "    #     'pooling': True,\n",
    "    # },\n",
    "    'cate_list_2': {\n",
    "        'feature_name': 'cate_list',\n",
    "        'preprocessor' : 'shared_embedding',\n",
    "        'shared_embedding': 'user_id',\n",
    "        'pooling': True,\n",
    "        'serving_input': tf.TensorSpec(shape=[None,None], dtype=tf.int64, name=\"cate_list_2\"),\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. define model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ToyModel(tf.keras.Model):\n",
    "    # 3.1 init vars\n",
    "    def __init__(self, config, preprocessoring_layer, embedding_size=None):\n",
    "        super().__init__()\n",
    "        self._embeddings = {}\n",
    "        self._config = config\n",
    "        self._average_pooling_1d = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self._average_pooling_2d = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        # 3.1.2 define input layer, typically embedding(index2emb)\n",
    "        for feature_name, feature_config in config.items():\n",
    "            preprocessor_type = feature_config['preprocessor']\n",
    "            layers = None\n",
    "            preprocessor = preprocessoring_layer.preprocessors[feature_name] # get info from preprocess stage\n",
    "            embedding_size = embedding_size if embedding_size is not None else feature_config.get('embedding_size', 8)\n",
    "            if preprocessor_type == 'hash':\n",
    "                layers = [tf.keras.layers.Embedding(feature_config['num_bins'], embedding_size, mask_zero=False,\n",
    "                                                embeddings_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                                name=f'{feature_name}_embedding')]\n",
    "                if feature_config.get('pooling'):\n",
    "                    layers.append(self._average_pooling_1d)\n",
    "            elif preprocessor_type == 'string':\n",
    "                layers = [\n",
    "                    tf.keras.layers.Embedding(preprocessor.vocabulary_size(), embedding_size, mask_zero=False, name=f'{feature_name}_embedding'),\n",
    "                ]\n",
    "                if feature_config.get('pooling'):\n",
    "                    layers.append(self._average_pooling_1d)\n",
    "            #                 layers = tf.keras.Sequential(layers, name=feature_name+'_bottom_'+preprocessor_type)\n",
    "            elif preprocessor_type == 'int':\n",
    "                layers = [\n",
    "                    tf.keras.layers.Embedding(preprocessor.vocabulary_size(), embedding_size, mask_zero=False, name=f'{feature_name}_embedding'),\n",
    "                ]\n",
    "                if feature_config.get('pooling'):\n",
    "                    layers.append(self._average_pooling_1d)\n",
    "            #                 layers = tf.keras.Sequential(layers, name=feature_name+'_bottom_'+preprocessor_type)\n",
    "            elif preprocessor_type == 'text':\n",
    "                layers = [\n",
    "                    tf.keras.layers.Embedding(preprocessor.vocabulary_size(), embedding_size, mask_zero=False, name=f'{feature_name}_embedding'),\n",
    "                    self._average_pooling_1d,\n",
    "                ]\n",
    "            elif preprocessor_type == 'norm':\n",
    "                layers = None\n",
    "            elif preprocessor_type == 'dis':\n",
    "                vocab_size = preprocessor.num_bins if preprocessor.num_bins else (len(preprocessor.bin_boundaries)+1)\n",
    "                layers = [\n",
    "                    tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=False, name=f'{feature_name}_embedding'),\n",
    "                ]\n",
    "            elif preprocessor_type == 'shared_embedding':\n",
    "                embedding_feature_name = feature_config['shared_embedding']\n",
    "                embedding = self._embeddings[embedding_feature_name].layers[0]\n",
    "                layers = [embedding]\n",
    "                if feature_config.get('pooling'):\n",
    "                    layers.append(self._average_pooling_2d if type(preprocessor) == tf.keras.layers.TextVectorization else self._average_pooling_1d)\n",
    "            self._embeddings[feature_name] = layers\n",
    "        # 3.1.3 define mlp layer\n",
    "        self._target_classification_layer_1 = tf.keras.layers.Dense(128,activation='relu',name='target_classification_layer_1')\n",
    "        self._target_classification_layer_2 = tf.keras.layers.Dense(1, activation='sigmoid', name='target_classification_layer_2')\n",
    "        self._target_regression_layer_1 = tf.keras.layers.Dense(128,activation='relu',name='target_regression_layer_1')\n",
    "        self._target_regression_layer_2 = tf.keras.layers.Dense(1, use_bias=True, name='target_regression_layer_2')\n",
    "        # 3.1.4 define target layer, especially for complex multi-task network\n",
    "        # omitted, todo\n",
    "        # 3.1.5 setup metrics\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = tf.keras.metrics.MeanAbsoluteError(name='mae_regression')\n",
    "        self.mse_metric = tf.keras.metrics.MeanSquaredError(name='mse_regression')\n",
    "        self.mape_metric = tf.keras.metrics.MeanAbsolutePercentageError(name='mape_regression')\n",
    "        self.msle_metric = tf.keras.metrics.MeanSquaredLogarithmicError(name='msle_regression')\n",
    "        self.auc_metric = tf.keras.metrics.AUC(name='auc_classification')\n",
    "        # or UDF metrics inherited from tf.keras.metrics.Metric\n",
    "\n",
    "    # 3.2 overwrite call function, define mapping (input: features --> output: predict)\n",
    "    # signature ref: https://stackoverflow.com/questions/60827999/use-dictionary-in-tf-function-input-signature-in-tensorflow-2-0\n",
    "    @tf.function(input_signature=[{feat_name:feat_conf[\"serving_input\"] for feat_name, feat_conf in feature_config.items()}])\n",
    "    def call(self, inputs):\n",
    "        # 3.2.1 from raw feature to embeddings, e.g. preprocessing, embedding lookup\n",
    "        embeddings = []\n",
    "        for feature_name, config in self._config.items():\n",
    "            # original_feature_name = config['feature_name'] if 'feature_name' in config and config['feature_name'] else feature_name\n",
    "            original_feature_name = feature_name\n",
    "            embedding_fn = self._embeddings[feature_name]\n",
    "            bottom = inputs[original_feature_name]\n",
    "            if isinstance(embedding_fn, list):\n",
    "                for layer in embedding_fn:\n",
    "                    bottom = layer(bottom)\n",
    "            elif isinstance(embedding_fn, tf.keras.Sequential):\n",
    "                bottom = embedding_fn(bottom)\n",
    "            elif config['preprocessor'] == 'norm' or not config['preprocessor']:\n",
    "                bottom = tf.reshape(tf.cast(bottom, tf.float32), [-1, 1])\n",
    "            bottom = tf.where(tf.math.is_nan(bottom), tf.zeros_like(bottom), bottom)\n",
    "            embeddings.append(bottom)\n",
    "        x = tf.concat(embeddings, axis=1)\n",
    "        # 3.2.2 go through network get predict values\n",
    "        target_classification = self._target_classification_layer_2(self._target_classification_layer_1(x))\n",
    "        target_regression = self._target_regression_layer_2(self._target_regression_layer_1(x))\n",
    "        return {\"target_classification\": target_classification, \"target_regression\": target_regression}\n",
    "    # 3.3 overwrite train_step function, input train data(x and label), output metrics(for tensorboard callback)\n",
    "    def train_step(self, data):\n",
    "        # 3.3.1 process label, e.g. duration to 0/1\n",
    "        x, y = data\n",
    "        y = self.process_label(y)\n",
    "        # 3.3.2 with tf.GradientTape() as tape: predict with model, then calculate losses\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y,y_pred) # passed when compile\n",
    "        # 3.3.3 gradients = tape.gradient(loss, trainable_vars); optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        trainable_vars = self.trainable_variables # self.trainable_variables: inherited attribute\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # self.optimizer: inherited attribute, assigned when compile\n",
    "        # 3.3.4 update metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.mse_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.msle_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.auc_metric.update_state(y[\"target_classification\"], y_pred[\"target_classification\"])\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    # 3.4 overwrite test_step(), same with train_step() except gradient\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y = self.process_label(y)\n",
    "        y_pred = self(x)\n",
    "        loss = self.compiled_loss(y,y_pred)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.mse_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.msle_metric.update_state(y[\"target_regression\"], y_pred[\"target_regression\"])\n",
    "        self.auc_metric.update_state(y[\"target_classification\"], y_pred[\"target_classification\"])\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    # 3.5 metrics, ref: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit?hl=en#going_lower-level\n",
    "    # def reset_metrics(self):\n",
    "    #     self.loss_tracker.reset_states()\n",
    "    #     self.mae_metric.reset_states()\n",
    "    #     self.mse_metric.reset_states()\n",
    "    #     self.msle_metric.reset_states()\n",
    "    #     self.auc_metric.reset_states()\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker,\n",
    "                self.mae_metric, self.mse_metric, self.msle_metric,\n",
    "            self.auc_metric\n",
    "        ]\n",
    "    def process_label(self,y):\n",
    "        y_target_classification = tf.reshape(y[\"target_classification\"], (-1,1))\n",
    "        y_target_regression_raw = y[\"target_regression\"] / tf.ones_like(y[\"target_regression\"])\n",
    "        y_target_regression = tf.clip_by_value(y_target_regression_raw, clip_value_min=0.0, clip_value_max=3.0)\n",
    "        return {\"target_classification\":y_target_classification,\"target_regression\":y_target_regression}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 define input description, parse raw input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# raw input, todo: complete and confirm\n",
    "# feature_description = {\n",
    "#     # features, created in form of float/int/string or array(float/int/string)\n",
    "#     'hash_sample': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "#     'int_sample': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "#     'norm_sample': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "#     'discrete_sample': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "#     'string_sample': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "#     'float_sample': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "#     'shared_embedding_sample': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "#     # 'fixed_len_seq_feature': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True, default_value=None) # must follow by pooling/attention layer before embed\n",
    "#     # 'ragged_sample': tf.io.RaggedFeature(tf.string), # must follow by pooling/attention layer before embed\n",
    "# }\n",
    "feature_description = {\n",
    "    # features, created in form of float/int/string or array(float/int/string)\n",
    "    'user_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'age': tf.io.FixedLenFeature([], tf.string, default_value='0'),\n",
    "    'gender': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'nickname': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'height': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "    'active_days': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'city': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'click': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'duration': tf.io.FixedLenFeature([], tf.float32, default_value=0),\n",
    "    'cate_list': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True, default_value=None),\n",
    "    # 'cate_list': tf.io.RaggedFeature(tf.string), # not compatible for model serving\n",
    "}\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_example(example_proto, feature_description)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 get a general preprocess object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age loaded ../vocab/test/age.vocab, vocab_size: 4\n",
      "cate_list loaded ../vocab/test/cates.vocab, vocab_size: 1003\n",
      "gender loaded ../vocab/test/gender.vocab, vocab_size: 1003\n",
      "features {\n",
      "  feature {\n",
      "    key: \"active_days\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"18\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"cate_list\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"film\"\n",
      "        value: \"music\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"city\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"shenzhen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"click\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"duration\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 3.200000047683716\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"gender\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"male\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"height\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 180.3000030517578\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"nickname\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"tommy\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"user_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"user_a\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 18:45:27.684898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "class PreprocessingLayer(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.preprocessors = {\n",
    "        }\n",
    "        self.config = config\n",
    "        for feature_name, config in self.config.items():\n",
    "            preprocessor = None\n",
    "            preprocessor_type = config['preprocessor']\n",
    "            if preprocessor_type == 'hash':\n",
    "                preprocessor = tf.keras.layers.Hashing(num_bins=config['num_bins'], mask_value=config['mask_value'], name=f'{feature_name}_{preprocessor_type}')\n",
    "            elif preprocessor_type == 'string':\n",
    "                preprocessor = tf.keras.layers.StringLookup(max_tokens=config['max_tokens'], num_oov_indices=config['num_oov_indices'],\n",
    "                                                            vocabulary=config.get('vocabulary'), name=f'{feature_name}_{preprocessor_type}')\n",
    "                if config.get('vocabulary'):\n",
    "                    print(f'{feature_name} loaded {config.get(\"vocabulary\")}, vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'int':\n",
    "                print(preprocessor_type,config['max_tokens'],config['num_oov_indices'],config.get('vocabulary'))\n",
    "                preprocessor = tf.keras.layers.IntegerLookup(max_tokens=config['max_tokens'], num_oov_indices=config['num_oov_indices'],\n",
    "                                                             vocabulary=config.get('vocabulary'), name=f'{feature_name}_{preprocessor_type}')\n",
    "                if config.get('vocabulary'):\n",
    "                    print(f'{feature_name} loaded {config.get(\"vocabulary\")}, vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'text':\n",
    "                preprocessor = tf.keras.layers.TextVectorization(max_tokens=config['max_tokens'], standardize=config['standardize'], split=config['split'],\n",
    "                                                                 vocabulary=config.get('vocabulary'), name=f'{feature_name}_{preprocessor_type}')\n",
    "                if config.get('vocabulary'):\n",
    "                    print(f'{feature_name} loaded {config.get(\"vocabulary\")}, vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'norm':\n",
    "                preprocessor = tf.keras.layers.Normalization(axis=None, mean=config.get('mean'), variance=config.get('variance'),\n",
    "                                                             name=f'{feature_name}_{preprocessor_type}')\n",
    "            elif preprocessor_type == 'dis':\n",
    "                if 'bin_boundaries' in config:\n",
    "                    preprocessor = tf.keras.layers.Discretization(bin_boundaries=config['bin_boundaries'], name=f'{feature_name}_{preprocessor_type}')\n",
    "                else:\n",
    "                    preprocessor = tf.keras.layers.Discretization(num_bins=config['num_bins'], name=f'{feature_name}_{preprocessor_type}')\n",
    "            elif preprocessor_type == 'shared_embedding':\n",
    "                shared_preprocessor_feature_name = config['shared_embedding']\n",
    "                preprocessor = self.preprocessors[shared_preprocessor_feature_name]\n",
    "            self.preprocessors[feature_name] = preprocessor\n",
    "        self.is_adapted=False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # if use string/text or other adapt demanding preprocessors, should assure is_adapted\n",
    "        # if not self.is_adapted:\n",
    "        #     raise RuntimeError('PreprocessingLayer is not adapted yet, call adapt() first.')\n",
    "        embeddings = {}\n",
    "        for feature_name, config in self.config.items():\n",
    "            original_feature_name = config['feature_name'] if 'feature_name' in config and config['feature_name'] else feature_name\n",
    "            preprocessor = self.preprocessors[feature_name]\n",
    "            if not preprocessor:\n",
    "                bottom = inputs[original_feature_name]\n",
    "            elif type(preprocessor) == tf.keras.layers.TextVectorization:\n",
    "                bottom = preprocessor(tf.expand_dims(inputs[original_feature_name], -1))\n",
    "            else:\n",
    "                bottom = preprocessor(inputs[original_feature_name])\n",
    "            embeddings[feature_name] = bottom\n",
    "        # set_trace()\n",
    "        #         x = tf.concat(embeddings, axis=1)\n",
    "        return embeddings\n",
    "\n",
    "    def adapt(self, dataset): # return a dict of feature_name: preprocessing_layer\n",
    "        for feature_name, config in self.config.items():\n",
    "            original_feature_name = config['feature_name'] if 'feature_name' in config and config['feature_name'] else feature_name\n",
    "            preprocessor_type = config['preprocessor']\n",
    "            preprocessor = self.preprocessors[feature_name]\n",
    "            if preprocessor_type in ('string', 'int', 'text') and config.get('vocabulary'):\n",
    "                print(f'{feature_name} loaded {config.get(\"vocabulary\")}, vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'norm' and config.get('mean') is not None and config.get('variance') is not None:\n",
    "                print(f'{feature_name} loaded with mean: {config.get(\"mean\")} variance: {config.get(\"variance\")}')\n",
    "            elif preprocessor_type == 'dis' and config.get('bin_boundaries'):\n",
    "                print(f'{feature_name} loaded with num_bins: {preprocessor.num_bins} bin_boundaries: {preprocessor.bin_boundaries}')\n",
    "            elif preprocessor_type == 'string':\n",
    "                preprocessor.adapt(dataset.map(lambda x: x[original_feature_name]))\n",
    "                print(f'finished {feature_name} vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'int':\n",
    "                preprocessor.adapt(dataset.map(lambda x: x[original_feature_name]))\n",
    "                print(f'finished {feature_name} vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'text':\n",
    "                preprocessor.adapt(dataset.map(lambda x: x[original_feature_name]))\n",
    "                print(f'finished {feature_name} vocab_size: {preprocessor.vocabulary_size()}')\n",
    "            elif preprocessor_type == 'norm':\n",
    "                preprocessor.adapt(dataset.map(lambda x: x[original_feature_name]))\n",
    "                print(f'finished {feature_name} mean: {preprocessor.mean} variance: {preprocessor.variance}')\n",
    "            elif preprocessor_type == 'dis':\n",
    "                preprocessor.adapt(dataset.map(lambda x: x[original_feature_name]))\n",
    "                print(f'finished {feature_name} num_bins: {preprocessor.num_bins} bin_boundaries: {preprocessor.bin_boundaries}')\n",
    "            else:\n",
    "                continue\n",
    "        self.is_adapted=True\n",
    "\n",
    "preprocessing_layer = PreprocessingLayer(feature_config)\n",
    "raw_dataset = tf.data.TFRecordDataset('part-r-00000.gz',  compression_type='GZIP')\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    print(tf.train.Example.FromString(raw_record.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 load from hdfs/local/..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_files_by_date_range(lastest_date_str, num_hours, data_dir, data_pattern):\n",
    "    DATE_FORMAT = '%Y-%m-%d/%H'\n",
    "    logging.info('The latest date is {}'.format(lastest_date_str))\n",
    "    lastest_date = datetime.strptime(lastest_date_str, DATE_FORMAT)\n",
    "    files = []\n",
    "    for hours_back in range(num_hours-1, -1, -1): # from new to old\n",
    "        curr_date = lastest_date - timedelta(hours=hours_back)\n",
    "        curr_date_str = curr_date.strftime(DATE_FORMAT)\n",
    "        dir_path = os.path.join(data_dir, curr_date_str)\n",
    "        logging.info(\"datadir={}\".format(dir_path))\n",
    "        if tf.io.gfile.exists(dir_path):\n",
    "            tfiles = tf.io.gfile.glob(dir_path + '/' + data_pattern)\n",
    "            logging.info(\"adding {} files at {}\".format(len(tfiles), dir_path))\n",
    "            files.extend(tfiles)\n",
    "    assert files\n",
    "    logging.info('Training over : %d files'%len(files))\n",
    "    return files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 prefetch, batch, map(parse with tf.io.parse_example), map(preprocessor(raw2index preprocessor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def make_dataset_remote(lastest_date, num_hours, batch_size=1,\n",
    "                        data_dir='hdfs://cluster/.../sample_tfrecord/', data_pattern='part-*'):\n",
    "    # training_files = get_files_by_date_range(lastest_date, num_hours, data_dir, data_pattern)\n",
    "    training_files = []\n",
    "    if tf.io.gfile.exists(\".\"):\n",
    "        tfiles = tf.io.gfile.glob('.' + '/' + data_pattern)\n",
    "        logging.info(\"adding {} files at {}\".format(len(tfiles), '.'))\n",
    "        training_files.extend(tfiles)\n",
    "    assert training_files\n",
    "    logging.info('Training over : %d files'%len(training_files))\n",
    "    print(f'{len(training_files)} training files.')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(training_files)\n",
    "    dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x, buffer_size=1, num_parallel_reads=1,\n",
    "                                                                   compression_type='GZIP'),\n",
    "                                 # cycle_length=4,\n",
    "                                 num_parallel_calls=tf.data.AUTOTUNE,\n",
    "                                 block_length=batch_size,\n",
    "                                 deterministic=False)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    # dataset = dataset.shuffle(buffer_size=10000) # optional\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # TODO: preprocessing_layer not integrated into serving model\n",
    "    dataset = dataset.map(lambda data: (preprocessing_layer(data), {\"target_classification\":data[\"click\"],\n",
    "                                                                     \"target_regression\":data[\"duration\"]}),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5 print(dataset.take(1)) for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training files.\n",
      "{'active_days_dis': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([3])>,\n",
      " 'age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'cate_list': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[808, 601]])>,\n",
      " 'cate_list_2': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[8, 1]])>,\n",
      " 'cate_list_hash': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[8, 1]])>,\n",
      " 'city': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([77])>,\n",
      " 'gender': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([499])>,\n",
      " 'height_dis': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n",
      " 'height_norm': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([10.300003], dtype=float32)>,\n",
      " 'user_id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([81])>}\n"
     ]
    }
   ],
   "source": [
    "dataset_train = make_dataset_remote('2022-01-20/14', num_hours=1, batch_size=1)\n",
    "for row in dataset_train.take(1):\n",
    "    pprint(row[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 compile model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_classification': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.17893597]], dtype=float32)>,\n",
      " 'target_regression': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-3.1409397]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "training_model = ToyModel(feature_config, preprocessing_layer)\n",
    "training_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss = {\"target_classification\":tf.losses.binary_crossentropy,\n",
    "            \"target_regression\":tf.losses.mean_squared_error,},\n",
    "    loss_weights = {\"target_classification\":1.5,\n",
    "                   \"target_regression\":1.0,},\n",
    "    #     run_eagerly=True\n",
    ")\n",
    "\n",
    "# check model output\n",
    "for row in dataset_train.take(1):\n",
    "    pprint(training_model(row[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 18:45:29.079630: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-05-11 18:45:29.079650: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-05-11 18:45:29.080104: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "log_dir = './tensorboard'\n",
    "tensorboard_callback_train = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    update_freq=100,\n",
    "    profile_batch=1000,\n",
    "    write_steps_per_second=True,\n",
    "    embeddings_freq=1\n",
    ")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    # Path where to save the model\n",
    "    # The two parameters below mean that we will overwrite\n",
    "    # the current checkpoint if and only if\n",
    "    # the `val_loss` score has improved.\n",
    "    # The saved model name will include the current epoch.\n",
    "    filepath=\"modelckpt\",\n",
    "    save_best_only=False,\n",
    "    verbose=1,\n",
    "    save_freq='epoch',\n",
    ")\n",
    "# custom callback, ref: https://www.tensorflow.org/guide/keras/custom_callback\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        # do something\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 model.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "      1/Unknown - 3s 3s/step - loss: 40.2922 - mae_regression: 6.1409 - mse_regression: 37.7111 - msle_regression: 1.9218 - auc_classification: 0.0000e+00\n",
      "Epoch 1: saving model to modelckpt\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 28.7432 - mae_regression: 4.2464 - mse_regression: 26.7424 - msle_regression: 0.9542 - auc_classification: 0.0000e+00\n",
      "Epoch 2/3\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 35.6655 - mae_regression: 5.7831 - mse_regression: 33.4439 - msle_regression: 1.9218 - auc_classification: 0.0000e+00\n",
      "Epoch 2: saving model to modelckpt\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n",
      "3/3 [==============================] - 3s 2s/step - loss: 14.9167 - mae_regression: 2.8802 - mse_regression: 13.7790 - msle_regression: 0.7349 - auc_classification: 0.5000 \n",
      "Epoch 3/3\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 32.0796 - mae_regression: 5.4890 - mse_regression: 30.1295 - msle_regression: 1.9218 - auc_classification: 0.0000e+00\n",
      "Epoch 3: saving model to modelckpt\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n",
      "3/3 [==============================] - 3s 1s/step - loss: 11.1735 - mae_regression: 2.0723 - mse_regression: 10.1669 - msle_regression: 0.6492 - auc_classification: 0.5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "2022-05-11 18:45:34.747822: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: modelckpt/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fe252ababb0>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model.fit(dataset_train, epochs=3, callbacks=[tensorboard_callback_train,checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. model.save(), if need call manually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model/ToyModel_20220511_184542/assets\n",
      "38572.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe2525b6790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model/ToyModel_20220511_184542/assets\n"
     ]
    }
   ],
   "source": [
    "model_name = f'model/{training_model.__class__.__name__}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "training_model.save(model_name)\n",
    "# print variable scale\n",
    "print(np.sum([np.prod(v.shape) for v in training_model.variables]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. model.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 23ms/step - loss: 10.6110 - mae_regression: 2.2530 - mse_regression: 9.6881 - msle_regression: 0.6997 - auc_classification: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": "[10.611014366149902,\n 2.2530229091644287,\n 9.688084602355957,\n 0.6996736526489258,\n 0.5]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = dataset_train\n",
    "tensorboard_callback_test = tensorboard_callback_train\n",
    "training_model.evaluate(dataset_test, callbacks=[tensorboard_callback_test])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. load model\n",
    "### 8.1 load entire model. may lose any custom train_step/test_step code. not recommend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# model_loaded = tf.keras.models.load_model(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.2 alternative option: define whole model and just load weights, with custom code reserved"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 18:45:48.523984: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open model/ToyModel_20220511_184542: FAILED_PRECONDITION: model/ToyModel_20220511_184542; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe2360ac9a0>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = ToyModel(feature_config, preprocessing_layer)\n",
    "model_loaded.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss = {\"target_classification\":tf.losses.binary_crossentropy,\n",
    "            \"target_regression\":tf.losses.mean_squared_error,},\n",
    "    loss_weights = {\"target_classification\":1.5,\n",
    "                   \"target_regression\":1.0,},\n",
    "    #     run_eagerly=True\n",
    ")\n",
    "model_loaded.load_weights(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. continue training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 5s 102ms/step - loss: 12.4165 - mae_regression: 2.7011 - mse_regression: 11.4950 - msle_regression: 1.0631 - auc_classification: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 12.0469 - mae_regression: 2.7240 - mse_regression: 11.2029 - msle_regression: 1.2812 - auc_classification: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 9.5862 - mae_regression: 2.3291 - mse_regression: 8.8145 - msle_regression: 0.8316 - auc_classification: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3/3 [==============================] - 1s 13ms/step - loss: 8.1002 - mae_regression: 2.0608 - mse_regression: 7.3924 - msle_regression: 0.7270 - auc_classification: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": "[8.100228309631348,\n 2.060763120651245,\n 7.392379283905029,\n 0.7269999980926514,\n 0.5]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.fit(dataset_train, epochs=3, callbacks=[tensorboard_callback_train])\n",
    "model_loaded.evaluate(dataset_test, callbacks=[tensorboard_callback_test])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. serving\n",
    "Ref: https://github.com/tensorflow/serving\n",
    "\n",
    "### 10.1 prepare docker env\n",
    "### 10.2 pull tensorflow serving image\n",
    "### 10.3 prepare model\n",
    "Prepare model files order by version\n",
    "\n",
    "Ref: https://stackoverflow.com/questions/45544928/tensorflow-serving-no-versions-of-servable-model-found-under-base-path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.4 check model signature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name='user_id'), 'age': TensorSpec(shape=(None,), dtype=tf.int64, name='age'), 'height_norm': TensorSpec(shape=(None,), dtype=tf.float32, name='height_norm'), 'height_dis': TensorSpec(shape=(None,), dtype=tf.int64, name='height_dis'), 'active_days_dis': TensorSpec(shape=(None,), dtype=tf.int64, name='active_days_dis'), 'city': TensorSpec(shape=(None,), dtype=tf.int64, name='city'), 'cate_list': TensorSpec(shape=(None, 2), dtype=tf.int64, name='cate_list'), 'cate_list_hash': TensorSpec(shape=(None, 2), dtype=tf.int64, name='cate_list_hash'), 'gender': TensorSpec(shape=(None,), dtype=tf.int64, name='gender'), 'cate_list_2': TensorSpec(shape=(None, 2), dtype=tf.int64, name='cate_list_2')}], {})\n",
      "ConcreteFunction signature_wrapper(*, height_norm, user_id, age, city, cate_list, active_days_dis, cate_list_hash, cate_list_2, height_dis, gender)\n",
      "  Args:\n",
      "    active_days_dis: int64 Tensor, shape=(None,)\n",
      "    age: int64 Tensor, shape=(None,)\n",
      "    cate_list: int64 Tensor, shape=(None, None)\n",
      "    cate_list_2: int64 Tensor, shape=(None, None)\n",
      "    cate_list_hash: int64 Tensor, shape=(None, None)\n",
      "    city: int64 Tensor, shape=(None,)\n",
      "    gender: int64 Tensor, shape=(None,)\n",
      "    height_dis: int64 Tensor, shape=(None,)\n",
      "    height_norm: float32 Tensor, shape=(None,)\n",
      "    user_id: int64 Tensor, shape=(None,)\n",
      "  Returns:\n",
      "    {'target_classification': <1>, 'target_regression': <2>}\n",
      "      <1>: float32 Tensor, shape=(None, 1)\n",
      "      <2>: float32 Tensor, shape=(None, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_model.save_spec())\n",
    "serving_model = tf.saved_model.load(model_name)\n",
    "print(serving_model.signatures[\"serving_default\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.5 start a container"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.6 double check model signature\n",
    "```bash\n",
    "curl http://${host}:${port}/v1/models/${model_name}/metadata\n",
    "```\n",
    "<details>\n",
    "  <summary>response, click to unfold</summary>\n",
    "  <pre><code class=\"language-json\">\n",
    "{\n",
    "    \"metadata\": {\n",
    "        \"signature_def\": {\n",
    "            \"signature_def\": {\n",
    "                \"__saved_model_init_op\": {\n",
    "                    \"inputs\": {},\n",
    "                    \"method_name\": \"\",\n",
    "                    \"outputs\": {\n",
    "                        \"__saved_model_init_op\": {\n",
    "                            \"dtype\": \"DT_INVALID\",\n",
    "                            \"name\": \"NoOp\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [],\n",
    "                                \"unknown_rank\": true\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"serving_default\": {\n",
    "                    \"inputs\": {\n",
    "                        \"active_days\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_active_days:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"age\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_age:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"cate_list\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_cate_list:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"cate_list_2\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_cate_list_2:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"cate_list_hash\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_cate_list_hash:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"city\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_city:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"gender\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_gender:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"height_dis\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_height_dis:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"height_norm\": {\n",
    "                            \"dtype\": \"DT_FLOAT\",\n",
    "                            \"name\": \"serving_default_height_norm:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"user_id\": {\n",
    "                            \"dtype\": \"DT_INT64\",\n",
    "                            \"name\": \"serving_default_user_id:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"method_name\": \"tensorflow/serving/predict\",\n",
    "                    \"outputs\": {\n",
    "                        \"target_classification\": {\n",
    "                            \"dtype\": \"DT_FLOAT\",\n",
    "                            \"name\": \"StatefulPartitionedCall:0\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        },\n",
    "                        \"target_regression\": {\n",
    "                            \"dtype\": \"DT_FLOAT\",\n",
    "                            \"name\": \"StatefulPartitionedCall:1\",\n",
    "                            \"tensor_shape\": {\n",
    "                                \"dim\": [\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"-1\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"\",\n",
    "                                        \"size\": \"1\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"unknown_rank\": false\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"model_spec\": {\n",
    "        \"name\": \"toy_model\",\n",
    "        \"signature_name\": \"\",\n",
    "        \"version\": \"1\"\n",
    "    }\n",
    "}\n",
    "  </code></pre>\n",
    "</details>\n",
    "\n",
    "<br/>\n",
    "Preprocessing_layer is not integrated into saved model in our case.\n",
    "Signatures are mapping to tensors before embedding layers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.7 predict\n",
    "\n",
    "Flawed example\n",
    "```bash\n",
    "curl -X POST -d \\\n",
    "'{\n",
    " \"instances\": [\n",
    "   {\n",
    "     \"user_id\": 44,\n",
    "     \"age\": 1,\n",
    "     \"height_norm\": 140.3,\n",
    "     \"height_dis\": 2,\n",
    "     \"active_days_dis\": 0,\n",
    "     \"city\": 67,\n",
    "     \"cate_list\": [638,522],\n",
    "     \"cate_list_hash\": [4,2],\n",
    "     \"gender\": 2,\n",
    "     \"cate_list_2\": [13,33]\n",
    "   },\n",
    "   {\n",
    "     \"user_id\": 23,\n",
    "     \"age\": 2,\n",
    "     \"height_norm\": 37.4,\n",
    "     \"height_dis\": 1,\n",
    "     \"active_days_dis\": 3,\n",
    "     \"city\": 98,\n",
    "     \"cate_list\": [11,11],\n",
    "     \"cate_list_hash\": [12,12],\n",
    "     \"gender\": 42,\n",
    "     \"cate_list_2\": [72] # attention!\n",
    "     # instances[1] wil fail cos shape(\"cate_list_2\") varies from instances[0]\n",
    "     # solution: pad seq features to same length, e.g. [72]->[72,0]\n",
    "   }\n",
    " ]\n",
    "}' http://${host}:${port}/v1/models/${model_name}:predict\n",
    "```\n",
    "\n",
    "response\n",
    "```json\n",
    "{\n",
    "    \"predictions\": [\n",
    "        {\n",
    "            \"target_classification\": [2.61704299e-05],\n",
    "            \"target_regression\": [-32.5374947]\n",
    "        },\n",
    "        {\n",
    "            \"target_classification\": [0.0522824526],\n",
    "            \"target_regression\": [-8.58270836]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.8 confirm serving predict result same to saved model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'target_classification': array([[2.6170430e-05],\n        [5.2282512e-02]], dtype=float32),\n 'target_regression': array([[-32.537495],\n        [ -8.582708]], dtype=float32)}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data = {\n",
    "                   \"user_id\": [[44],[23]],\n",
    "                   \"age\": [[1],[2]],\n",
    "                   \"height_norm\": [[140.3],[37.4]],\n",
    "                   \"height_dis\": [[2],[1]],\n",
    "                   \"active_days_dis\": [[0],[3]],\n",
    "                   \"city\": [[67],[98]],\n",
    "                   \"cate_list\": [[[638,522]],[[11,11]]],\n",
    "                   \"cate_list_hash\": [[[4,2]],[[12,12]]],\n",
    "                   \"gender\": [[2],[42]],\n",
    "                   \"cate_list_2\": [[[13,33]],[[72,0]]]\n",
    "               }\n",
    "training_model.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(predict_data).map(lambda x:{\n",
    "        \"user_id\":tf.cast(x[\"user_id\"],tf.int64),\n",
    "        \"age\":tf.cast(x[\"age\"],tf.int64),\n",
    "        \"height_norm\":tf.cast(x[\"height_norm\"],tf.float32),\n",
    "        \"height_dis\":tf.cast(x[\"height_dis\"],tf.int64),\n",
    "        \"active_days_dis\":tf.cast(x[\"active_days_dis\"],tf.int64),\n",
    "        \"city\":tf.cast(x[\"city\"],tf.int64),\n",
    "        \"cate_list\":tf.cast(x[\"cate_list\"],tf.int64),\n",
    "        \"cate_list_hash\":tf.cast(x[\"cate_list_hash\"],tf.int64),\n",
    "        \"gender\":tf.cast(x[\"gender\"],tf.int64),\n",
    "        \"cate_list_2\":tf.cast(x[\"cate_list_2\"],tf.int64),\n",
    "    })\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}